# RAG 智能助手数据集选型与构建建议书

根据 `database_selection.md` 的分析以及本项目“上传私有文档进行检索问答”的核心需求，我们建议采用以下分层数据集构建策略。

## 1. 核心测试场景与推荐数据集

### 1.1 单文档精确问答 (基础能力)
**场景描述**：用户上传一份说明书或合同，询问其中的具体条款或参数。
**推荐参考**：**SQuAD 2.0**
- **理由**：SQuAD 2.0 包含“无法回答”的问题，非常适合测试 Agent 在文档中找不到信息时是否会产生幻觉。
- **构建建议**：
    - 选取 10-20 篇内部技术文档（PDF/Word）。
    - 每篇文档人工标注 5 个“有答案”问题和 2 个“无答案”问题。

### 1.2 多文档/跨文档推理 (进阶能力)
**场景描述**：用户上传了多份会议纪要或多代产品手册，询问“今年和去年的策略有什么不同”。
**推荐参考**：**HotpotQA**
- **理由**：专门针对多跳推理（Multi-hop Reasoning），需要跨越多个段落才能找到完整答案。
- **构建建议**：
    - 构建“冲突型”数据集（如我们之前的 `NebulaTech` 案例）。
    - 必须包含需要结合两处信息才能回答的问题。

### 1.3 对话式检索 (真实交互)
**场景描述**：用户在多轮对话中逐步追问信息。
**推荐参考**：**MultiDoc2Dial**
- **理由**：模拟客服/助手场景，不仅考查检索，还考查对上下文意图的理解。
- **构建建议**：
    - 录制或编写 5 组多轮对话脚本，测试 Agent 是否能记住上一轮检索的上下文。

## 2. 本项目测试数据集生成计划

我们将基于 DeepSeek 大模型能力，自动生成一套模拟上述特征的测试集，包含以下文件：

1.  **`knowledge_corpus.json`**：模拟的私有知识库（包含技术文档、会议纪要等片段）。
2.  **`test_dataset.json`**：测试用例集，包含：
    - `question`: 用户问题
    - `reference_answer`: 标准答案
    - `source_doc_ids`: 答案依据的文档ID
    - `type`: 问题类型（单跳、多跳、无答案、冲突）

## 3. 评估维度映射

- **准确性 (Faithfulness)**: 回答是否严格忠实于上传的文档？ -> 参考 SQuAD/RAGAS 指标
- **召回率 (Recall)**: 是否找到了所有相关文档？ -> 参考 MS MARCO 指标
- **抗噪性 (Noise Robustness)**: 面对无关文档是否会受干扰？ -> 参考 TriviaQA 特性
